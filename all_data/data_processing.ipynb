{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "class Vocab(object):\n",
    "    unk = u'<unk>'\n",
    "    sos = u'<sos>'\n",
    "    eos = u'<eos>'\n",
    "    def __init__(self, unk=unk):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.word_freq = defaultdict(int)\n",
    "        self.total_words = 0\n",
    "        self.unknown = unk\n",
    "        self.add_word(self.unknown, count=0)\n",
    "        self.add_word(self.sos, count=0)\n",
    "        self.add_word(self.eos, count=0)\n",
    "\n",
    "    def add_word(self, word, count=1):\n",
    "        word = word.strip()\n",
    "        if len(word) == 0:\n",
    "            return\n",
    "        elif word.isspace():\n",
    "            return\n",
    "        if word not in self.word_to_index:\n",
    "            index = len(self.word_to_index)\n",
    "            self.word_to_index[word] = index\n",
    "            self.index_to_word[index] = word\n",
    "        self.word_freq[word] += count\n",
    "\n",
    "        \n",
    "    def construct(self, words):\n",
    "        for word in words:\n",
    "            self.add_word(word)\n",
    "        self.total_words = float(sum(self.word_freq.values()))\n",
    "        print '{} total words with {} uniques'.format(self.total_words, len(self.word_freq))\n",
    " \n",
    "\n",
    "    def limit_vocab_length(self, length):\n",
    "        \"\"\"\n",
    "        Truncate vocabulary to keep most frequent words\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            None \n",
    "        \"\"\"\n",
    "        if length > self.__len__():\n",
    "            return\n",
    "        new_word_to_index = {self.unknown:0}\n",
    "        new_index_to_word = {0:self.unknown}\n",
    "        self.word_freq.pop(self.unknown)          #pop unk word\n",
    "        sorted_tup = sorted(self.word_freq.items(), key=operator.itemgetter(1))\n",
    "        sorted_tup.reverse()\n",
    "        vocab_tup = sorted_tup[:length]\n",
    "        self.word_freq = dict(vocab_tup)\n",
    "        for word in self.word_freq:\n",
    "            index = len(new_word_to_index)\n",
    "            new_word_to_index[word] = index\n",
    "            new_index_to_word[index] = word\n",
    "        self.word_to_index = new_word_to_index\n",
    "        self.index_to_word = new_index_to_word\n",
    "        self.word_freq[self.unknown]=0\n",
    "        \n",
    "        \n",
    "    def save_vocab(self, filePath):\n",
    "        \"\"\"\n",
    "        Save vocabulary a offline file\n",
    "        \n",
    "        Args:\n",
    "            filePath: where you want to save your vocabulary, every line in the \n",
    "            file represents a word with a tab seperating word and it's frequency\n",
    "            \n",
    "        Returns:\n",
    "            None \n",
    "        \"\"\"\n",
    "        self.word_freq.pop(self.unknown)\n",
    "        sorted_tup = sorted(self.word_freq.items(), key=operator.itemgetter(1))\n",
    "        sorted_tup.reverse()\n",
    "        with open(filePath, 'wb') as fd:\n",
    "            for (word, freq) in sorted_tup:\n",
    "                fd.write(('%s\\t%d\\n'%(word, freq)).encode('utf-8'))\n",
    "            \n",
    "\n",
    "    def load_vocab_from_file(self, filePath, sep='\\t'):\n",
    "        \"\"\"\n",
    "        Truncate vocabulary to keep most frequent words\n",
    "        \n",
    "        Args:\n",
    "            filePath: vocabulary file path, every line in the file represents \n",
    "                a word with a tab seperating word and it's frequency\n",
    "            \n",
    "        Returns:\n",
    "            None \n",
    "        \"\"\"\n",
    "        with open(filePath, 'rb') as fd:\n",
    "            for line in fd:\n",
    "                line_uni = line.decode('utf-8')\n",
    "                word, freq = line_uni.split(sep)\n",
    "                index = len(self.word_to_index)\n",
    "                if word not in self.word_to_index:\n",
    "                    self.word_to_index[word] = index\n",
    "                    self.index_to_word[index] = word\n",
    "                self.word_freq[word] = int(freq)\n",
    "            print 'load from <'+filePath+'>, there are {} words in dictionary'.format(len(self.word_freq))\n",
    " \n",
    "\n",
    "    def encode(self, word):\n",
    "        if word not in self.word_to_index:\n",
    "            word = self.unknown\n",
    "        return self.word_to_index[word]\n",
    "\n",
    "    \n",
    "    def decode(self, index):\n",
    "        return self.index_to_word[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train.txt','r') as fd:\n",
    "    step=0\n",
    "    for line in fd:\n",
    "        line_uni = line.decode('utf-8')\n",
    "        if step < 2:\n",
    "            step+=1\n",
    "            continue\n",
    "        step+=1\n",
    "        if line_uni.isspace():\n",
    "            step=0\n",
    "            continue\n",
    "        for word in line_uni.strip().split():\n",
    "            vocab.add_word(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test.txt','r') as fd:\n",
    "    step=0\n",
    "    for line in fd:\n",
    "        line_uni = line.decode('utf-8')\n",
    "        if step < 2:\n",
    "            step+=1\n",
    "            continue\n",
    "        step+=1\n",
    "        if line_uni.isspace():\n",
    "            step=0\n",
    "            continue\n",
    "        for word in line_uni.strip().split():\n",
    "            vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('valid.txt','r') as fd:\n",
    "    step=0\n",
    "    for line in fd:\n",
    "        line_uni = line.decode('utf-8')\n",
    "        if step < 2:\n",
    "            step+=1\n",
    "            continue\n",
    "        step+=1\n",
    "        if line_uni.isspace():\n",
    "            step=0\n",
    "            continue\n",
    "        for word in line_uni.strip().split():\n",
    "            vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab.save_vocab('vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from <vocab.txt>, there are 2718261 words in dictionary\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "vocab.load_vocab_from_file('vocab.txt')\n",
    "vocab.limit_vocab_length(100000)\n",
    "vocab.save_vocab('vocab.100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def load_data(fileName):\n",
    "    with open(fileName,'r') as fd:\n",
    "        step=0\n",
    "        data_list = []\n",
    "        tmp_data = []\n",
    "        for line in fd:\n",
    "            line_uni = line.decode('utf-8')\n",
    "            if step < 2:\n",
    "                tmp_data = []\n",
    "                step+=1\n",
    "                continue\n",
    "            step+=1\n",
    "            if line_uni.isspace():\n",
    "                step=0\n",
    "                data_list.append(tmp_data)\n",
    "                continue\n",
    "            tmp_data.append(line_uni.strip().split())\n",
    "    return data_list\n",
    "\n",
    "def flatten(li):\n",
    "    ret = []\n",
    "    for item in li:\n",
    "        if isinstance(item, list) or isinstance(item, tuple):\n",
    "            ret += flatten(item)\n",
    "        else:\n",
    "            ret.append(item)\n",
    "    return ret\n",
    "\n",
    "def shuffleData(data):\n",
    "    def shuffleList(li):\n",
    "        li = copy.deepcopy(li)\n",
    "        index = range(len(li))\n",
    "        np.random.shuffle(index)\n",
    "        tmp_list = [li[i] for i in index]\n",
    "        index = np.argsort(index)\n",
    "        return tmp_list, index.tolist()\n",
    "    ret_data=[]\n",
    "    ret_label = []\n",
    "    for item in data:\n",
    "        shuffled, label = shuffleList(item)\n",
    "        ret_data.append(shuffled)\n",
    "        ret_label.append(label)\n",
    "    return ret_data, ret_label\n",
    "        \n",
    "\n",
    "def batch_encodeNpad(data, label, vocab):\n",
    "    sent_num = [len(i) for i in data]\n",
    "    max_sent_num = max(sent_num)\n",
    "    sent_len = [[len(i[j]) if j<len(i) else 0 for j in range(max_sent_num)]for i in data]\n",
    "    max_sent_len = max(flatten(sent_len))\n",
    "    ret_label = [[i[j] if j<len(i) else -1 for j in range(max_sent_num)] for i in label]\n",
    "    ret_batch = np.zeros([len(data), max_sent_num, max_sent_len], dtype=np.int32)\n",
    "    for (i, item) in enumerate(data):\n",
    "        for (j, sent) in enumerate(item):\n",
    "            for (k, word) in enumerate(sent):\n",
    "                ret_batch[i, j, k] = vocab.encode(word)\n",
    "    return ret_batch, np.array(ret_label), sent_num, sent_len  #(b_sz, max_snum, max_slen), (b_sz, max_snum), (b_sz,), (max_slen)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from <vocab.100k>, there are 100003 words in dictionary\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'list'>\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "vocab = Vocab()\n",
    "vocab.load_vocab_from_file('vocab.100k')\n",
    "data_list = load_data('test_file')\n",
    "ret_data, ret_label = shuffleData(data_list)\n",
    "ret = batch_encodeNpad(ret_data, ret_label, vocab)\n",
    "print type(ret[0])\n",
    "print type(ret[1])\n",
    "print type(ret[2])\n",
    "print type(ret[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 2]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "aa = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "np.random.shuffle(aa[0])\n",
    "print aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 2, 1, 4]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = [1, 2, 3, 4, 5]\n",
    "np.random.shuffle(aa)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 6, 3, 0, 9, 5, 4, 1, 7, 2]\n",
      "[3 7 9 2 6 5 1 8 0 4]\n"
     ]
    }
   ],
   "source": [
    "index = range(10)\n",
    "np.random.shuffle(index)\n",
    "print index\n",
    "print np.argsort(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2]\n"
     ]
    }
   ],
   "source": [
    "aa = np.array([[4, 2, 3, 1], [4, 3, 2, 1]])\n",
    "print  aa[0, [3, 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 4]\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "li = [1, 2, 3, 4]\n",
    "aa = li\n",
    "li = copy.deepcopy(li)\n",
    "np.random.shuffle(li)\n",
    "print li\n",
    "print aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffleData(data):\n",
    "    def shuffleList(li):\n",
    "        li = copy.deepcopy(li)\n",
    "        index = range(len(li))\n",
    "        np.random.shuffle(index)\n",
    "        tmp_list = [li[i] for i in index]\n",
    "        index = np.argsort(index)\n",
    "        return tmp_list, index.tolist()\n",
    "    ret_data=[]\n",
    "    ret_label = []\n",
    "    for item in data:\n",
    "        shuffled, label = shuffleList(item)\n",
    "        ret_data.append(shuffled)\n",
    "        ret_label.append(label)\n",
    "    return ret_data, ret_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[3, 5, 4, 2, 1], [2, 4, 6, 3, 7, 5], [9, 6, 8, 5, 7]], [[4, 3, 0, 2, 1], [0, 3, 1, 5, 2, 4], [3, 1, 4, 2, 0]])\n",
      "[[1, 2, 3, 4, 5], [2, 3, 4, 5, 6, 7], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "aa = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6, 7], [5, 6, 7, 8, 9]]\n",
    "print shuffleData(aa)\n",
    "print aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.array([1, 2, 3, 4, 5])\n",
    "aa.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 0, 1, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = [0, 1, 2]\n",
    "np.concatenate([aa, bb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
